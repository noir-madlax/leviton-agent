import json
import os
import asyncio
import aiofiles
import pandas as pd
from datetime import datetime, timedelta
from apify_client import ApifyClient
from typing import Dict, Any, List, Optional
import logging

from core.database.connection import get_supabase_service_client
from core.repositories.amazon_product_repository import AmazonProductRepository

logger = logging.getLogger(__name__)

# è·å–scrapingæ¨¡å—çš„dataç›®å½•è·¯å¾„
SCRAPING_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_DIR = os.path.join(SCRAPING_DIR, "data")
OUTPUT_DIR = os.path.join(DATA_DIR, "scraped")
AMAZON_REVIEW_DIR = os.path.join(OUTPUT_DIR, "amazon", "review")

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(AMAZON_REVIEW_DIR, exist_ok=True)

# Apify API Configuration
APIFY_API_TOKEN = os.getenv("APIFY_API_TOKEN")
AXESSO_ACTOR_ID = "ZebkvH3nVOrafqr5T"

# Amazon Review Scraper Configuration (constant across all requests)
MAX_PAGES_PER_ASIN = 5  # Maximum pages to scrape per ASIN in single API call
DEFAULT_REVIEWS_PER_PAGE = 10  # Default number of reviews per page when currentPage is missing

AMAZON_REVIEW_CONFIG = {
    "domainCode": "com",           # Amazon.com (US marketplace)
    "sortBy": "recent",            # Sort by most recent reviews
    "maxPages": MAX_PAGES_PER_ASIN,  # Maximum pages per request
    "filterByStar": None,          # No star filter (all ratings)
    "filterByKeyword": None,       # No keyword filter
    "reviewerType": "verified_reviews",  # Only verified purchase reviews
    "formatType": "current_format",      # Use current Amazon review format
    "mediaType": "all_contents"          # Include all content types
}

# Configuration constants
RATE_LIMIT_DELAY = 0.1  # seconds - 100ms delay between individual requests
MAX_CONCURRENT_REQUESTS = 32  # Maximum concurrent API requests
SCRAPE_RECENCY_DAYS = 30  # Days to consider a scrape "recent"
DAYS_PER_MONTH = 30  # Approximation for months to days conversion
DEFAULT_COVERAGE_MONTHS = 6  # Default months of review coverage required

# Review date filtering constants
MIN_REVIEW_YEAR = 2010  # Earliest acceptable review year
MAX_REVIEW_YEAR = datetime.now().year  # Latest acceptable review year (current year)


class ReviewScraper:
    """è¯„è®ºçˆ¬å–å™¨ - è´Ÿè´£Amazonè¯„è®ºæ•°æ®çš„çˆ¬å–"""
    
    def __init__(self):
        self.review_dir = AMAZON_REVIEW_DIR
    
    async def scrape_for_batch(self, batch_id: int, review_coverage_months: int = DEFAULT_COVERAGE_MONTHS) -> Dict[str, Any]:
        """
        ä¸ºç‰¹å®šæ‰¹æ¬¡çš„äº§å“çˆ¬å–è¯„è®ºæ•°æ®
        
        Args:
            batch_id: æ‰¹æ¬¡ID (å¯¹åº”scraping_requests.id)
            review_coverage_months: è¯„è®ºè¦†ç›–æœˆæ•°
            
        Returns:
            Dict[str, Any]: çˆ¬å–ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹ä¸ºæ‰¹æ¬¡ {batch_id} çˆ¬å–è¯„è®ºæ•°æ®...")
        
        try:
            # è·å–æ•°æ®åº“å®¢æˆ·ç«¯
            supabase_client = get_supabase_service_client()
            product_repository = AmazonProductRepository(supabase_client)
            
            # ä»æ•°æ®åº“è·å–è¯¥æ‰¹æ¬¡çš„äº§å“åˆ—è¡¨
            logger.info(f"ğŸ“Š ä»æ•°æ®åº“è·å–æ‰¹æ¬¡ {batch_id} çš„äº§å“åˆ—è¡¨...")
            products = await product_repository.get_products_by_batch(batch_id)
            
            if not products:
                logger.warning(f"âš ï¸  æ‰¹æ¬¡ {batch_id} ä¸­æ²¡æœ‰æ‰¾åˆ°äº§å“æ•°æ®")
                return {
                    "status": "warning",
                    "message": f"æ‰¹æ¬¡ {batch_id} ä¸­æ²¡æœ‰äº§å“æ•°æ®",
                    "batch_id": batch_id,
                    "total_asins": 0,
                    "successful": 0,
                    "skipped": 0,
                    "errors": 0
                }
            
            # æå–ASINåˆ—è¡¨
            asins = []
            for product in products:
                platform_id = product.get('platform_id')
                if platform_id:
                    asins.append(platform_id)
            
            logger.info(f"ğŸ“¦ æ‰¾åˆ° {len(asins)} ä¸ªASINéœ€è¦çˆ¬å–è¯„è®º")
            
            if not asins:
                logger.warning(f"âš ï¸  æ‰¹æ¬¡ {batch_id} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ASIN")
                return {
                    "status": "warning", 
                    "message": f"æ‰¹æ¬¡ {batch_id} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ASIN",
                    "batch_id": batch_id,
                    "total_asins": 0,
                    "successful": 0,
                    "skipped": 0,
                    "errors": 0
                }
            
            logger.info(f"\nğŸ“‹ è¯„è®ºçˆ¬å–é…ç½®:")
            logger.info(f"   â€¢ æ‰¹æ¬¡ID: {batch_id}")
            logger.info(f"   â€¢ ASINæ•°é‡: {len(asins)}")
            logger.info(f"   â€¢ è¯„è®ºè¦†ç›–æœˆæ•°: {review_coverage_months}")
            logger.info(f"   â€¢ æœ€å¤§å¹¶å‘è¯·æ±‚: {MAX_CONCURRENT_REQUESTS}")
            logger.info(f"   â€¢ æ¯ä¸ªASINæœ€å¤§é¡µæ•°: {MAX_PAGES_PER_ASIN}")
            logger.info(f"   â€¢ æ•°æ®ä¿å­˜ç›®å½•: {AMAZON_REVIEW_DIR}")
            
            # åˆ›å»ºä¿¡å·é‡è¿›è¡Œå¹¶å‘æ§åˆ¶
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
            
            # ä¸ºæ‰€æœ‰ASINåˆ›å»ºçˆ¬å–ä»»åŠ¡
            tasks = [
                self._scrape_product_reviews(asin, semaphore, review_coverage_months, batch_id) 
                for asin in asins
            ]
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ç»Ÿè®¡ç»“æœ
            successful = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "success")
            skipped = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "skipped")
            errors = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "error")
            exceptions = sum(1 for r in results if not isinstance(r, dict))
            
            logger.info(f"\nğŸ“Š æ‰¹æ¬¡ {batch_id} è¯„è®ºçˆ¬å–æ±‡æ€»:")
            logger.info(f"   âœ… æˆåŠŸ: {successful}")
            logger.info(f"   â© è·³è¿‡: {skipped}")
            logger.info(f"   âŒ é”™è¯¯: {errors}")
            logger.info(f"   ğŸš« å¼‚å¸¸: {exceptions}")
            logger.info(f"   ğŸ“ æ–‡ä»¶ä¿å­˜ä½ç½®: {AMAZON_REVIEW_DIR}")
            
            # ç”Ÿæˆæ‰¹æ¬¡æ±‡æ€»æŠ¥å‘Š
            summary_data = {
                "batch_scrape_session": {
                    "timestamp": datetime.now().isoformat(),
                    "batch_id": batch_id,
                    "total_asins": len(asins),
                    "successful": successful,
                    "skipped": skipped,
                    "errors": errors,
                    "exceptions": exceptions,
                    "review_coverage_months": review_coverage_months,
                    "configuration": {
                        "rate_limit_delay": RATE_LIMIT_DELAY,
                        "max_concurrent_requests": MAX_CONCURRENT_REQUESTS,
                        "scrape_recency_days": SCRAPE_RECENCY_DAYS,
                        "max_pages_per_asin": MAX_PAGES_PER_ASIN,
                        "min_review_year": MIN_REVIEW_YEAR,
                        "max_review_year": MAX_REVIEW_YEAR
                    }
                },
                "asins_list": asins,
                "results": [r for r in results if isinstance(r, dict)]
            }
            
            # ä¿å­˜æ‰¹æ¬¡æ±‡æ€»
            summary_file = os.path.join(
                AMAZON_REVIEW_DIR, 
                f"batch_{batch_id}_review_scrape_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            async with aiofiles.open(summary_file, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(summary_data, indent=2, ensure_ascii=False))
            
            logger.info(f"ğŸ“„ æ‰¹æ¬¡æ±‡æ€»ä¿å­˜åˆ°: {summary_file}")
            
            # åˆ¤æ–­æ•´ä½“çŠ¶æ€
            if successful + skipped == len(asins):
                status = "success"
                message = f"æ‰¹æ¬¡ {batch_id} è¯„è®ºçˆ¬å–å®Œæˆ"
            elif successful > 0:
                status = "partial_success"
                message = f"æ‰¹æ¬¡ {batch_id} è¯„è®ºçˆ¬å–éƒ¨åˆ†æˆåŠŸ"
            else:
                status = "error"
                message = f"æ‰¹æ¬¡ {batch_id} è¯„è®ºçˆ¬å–å¤±è´¥"
            
            return {
                "status": status,
                "message": message,
                "batch_id": batch_id,
                "total_asins": len(asins),
                "successful": successful,
                "skipped": skipped,
                "errors": errors,
                "exceptions": exceptions,
                "summary_file": summary_file,
                "data_directory": AMAZON_REVIEW_DIR
            }
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡ {batch_id} è¯„è®ºçˆ¬å–æ—¶å‡ºç°å¼‚å¸¸: {e}", exc_info=True)
            return {
                "status": "error",
                "message": f"æ‰¹æ¬¡ {batch_id} è¯„è®ºçˆ¬å–å¼‚å¸¸: {str(e)}",
                "batch_id": batch_id,
                "total_asins": 0,
                "successful": 0,
                "skipped": 0,
                "errors": 0
            }
    
    async def _scrape_product_reviews(self, asin: str, semaphore: asyncio.Semaphore, 
                                     review_coverage_months: int, batch_id: Optional[int] = None) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªäº§å“çš„è¯„è®º
        
        Args:
            asin: äº§å“ASIN
            semaphore: å¹¶å‘æ§åˆ¶ä¿¡å·é‡
            review_coverage_months: è¯„è®ºè¦†ç›–æœˆæ•°
            batch_id: æ‰¹æ¬¡IDï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, Any]: çˆ¬å–ç»“æœ
        """
        async with semaphore:
            try:
                logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†ASIN: {asin}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è·³è¿‡
                skip_action = await self._determine_skip_action(asin, review_coverage_months)
                
                if skip_action["should_skip"]:
                    logger.info(f"â© è·³è¿‡ASIN {asin}: {skip_action['reason']}")
                    return {
                        "status": "skipped",
                        "asin": asin,
                        "reason": skip_action["reason"],
                        "message": f"ASIN {asin} è¢«è·³è¿‡: {skip_action['reason']}"
                    }
                
                # çˆ¬å–è¯„è®º
                logger.info(f"ğŸ” çˆ¬å–ASIN {asin} çš„è¯„è®º...")
                reviews_data = await asyncio.to_thread(self._get_amazon_reviews_apify, asin)
                
                if not reviews_data.get("success"):
                    error_msg = reviews_data.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ ASIN {asin} çˆ¬å–å¤±è´¥: {error_msg}")
                    return {
                        "status": "error",
                        "asin": asin,
                        "error": error_msg,
                        "message": f"ASIN {asin} çˆ¬å–å¤±è´¥: {error_msg}"
                    }
                
                # ç”Ÿæˆæ–‡ä»¶åå¹¶ä¿å­˜
                filepath = await self._generate_filename(asin, batch_id)
                
                # åˆ†ææ˜¯å¦è·å–äº†æœ€æ—©çš„è¯„è®º
                earliest_reviews_fetched = False
                if reviews_data.get("fetched_all_available_reviews"):
                    earliest_reviews_fetched = True
                
                # ä¿å­˜è¯„è®ºæ•°æ®
                await self._save_reviews_with_context(
                    asin, reviews_data, filepath, 
                    earliest_reviews_fetched=earliest_reviews_fetched,
                    max_pages_info={
                        "max_pages_requested": reviews_data.get("max_pages_requested", MAX_PAGES_PER_ASIN),
                        "max_pages_reached": reviews_data.get("max_pages_reached", 0)
                    }
                )
                
                reviews_count = reviews_data.get("total_reviews", 0)
                logger.info(f"âœ… ASIN {asin} å®Œæˆ: {reviews_count} æ¡è¯„è®ºå·²ä¿å­˜åˆ° {filepath}")
                
                # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                if RATE_LIMIT_DELAY > 0:
                    await asyncio.sleep(RATE_LIMIT_DELAY)
                
                return {
                    "status": "success",
                    "asin": asin,
                    "reviews_count": reviews_count,
                    "filepath": filepath,
                    "message": f"ASIN {asin} æˆåŠŸçˆ¬å– {reviews_count} æ¡è¯„è®º"
                }
                
            except Exception as e:
                logger.error(f"ğŸš« ASIN {asin} å¤„ç†å¼‚å¸¸: {str(e)}")
                return {
                    "status": "error",
                    "asin": asin,
                    "error": str(e),
                    "message": f"ASIN {asin} å¤„ç†å¼‚å¸¸: {str(e)}"
                }
    
    async def _get_existing_review_files(self, asin: str) -> List[str]:
        """è·å–ASINå·²æœ‰çš„è¯„è®ºæ–‡ä»¶"""
        existing_files = []
        for filename in os.listdir(AMAZON_REVIEW_DIR):
            # æ£€æŸ¥æ—§å‘½åçº¦å®š
            if filename.startswith(f"{asin}_") and filename.endswith("_reviews.json"):
                existing_files.append(os.path.join(AMAZON_REVIEW_DIR, filename))
            # æ£€æŸ¥æ–°å‘½åçº¦å®š
            elif filename.startswith("batch_") and f"_{asin}_" in filename and filename.endswith("_reviews.json"):
                existing_files.append(os.path.join(AMAZON_REVIEW_DIR, filename))
        return existing_files
    
    def _get_amazon_reviews_apify(self, asin: str) -> Dict[str, Any]:
        """ä½¿ç”¨Apify APIè·å–Amazonäº§å“è¯„è®º"""
        if not APIFY_API_TOKEN:
            raise ValueError("APIFY_API_TOKEN environment variable is required")
        
        # åˆå§‹åŒ–ApifyClient
        client = ApifyClient(APIFY_API_TOKEN)
        
        # åˆ›å»ºè¾“å…¥é…ç½®
        config = AMAZON_REVIEW_CONFIG.copy()
        config["asin"] = asin
        
        run_input = {"input": [config]}
        
        try:
            logger.info(f"   ğŸ”„ å¯åŠ¨Apify actor for ASIN {asin}, max_pages={MAX_PAGES_PER_ASIN}...")
            run = client.actor(AXESSO_ACTOR_ID).call(run_input=run_input)
            
            # ä»è¿è¡Œçš„æ•°æ®é›†è·å–ç»“æœ
            all_items = []
            for item in client.dataset(run["defaultDatasetId"]).iterate_items():
                all_items.append(item)
            
            if not all_items:
                return {
                    "reviews": [],
                    "total_reviews": 0,
                    "max_pages_requested": MAX_PAGES_PER_ASIN,
                    "max_pages_reached": 0,
                    "fetched_all_available_reviews": True,
                    "success": True
                }
            
            # æå–è¯„è®ºå¹¶ç¡®å®šå®é™…åˆ°è¾¾çš„é¡µæ•°
            all_reviews = []
            max_current_page = 0
            has_current_page_info = False
            
            for item in all_items:
                # æ¯ä¸ªitemåº”è¯¥æœ‰currentPageå¹¶ä¸”æ˜¯ä¸€ä¸ªè¯„è®º
                if "currentPage" in item:
                    current_page = item.get("currentPage", 1)
                    max_current_page = max(max_current_page, current_page)
                    has_current_page_info = True
                
                # å¦‚æœitemæœ‰è¯„è®ºå­—æ®µï¼Œåˆ™è¯¥itemæœ¬èº«å°±æ˜¯ä¸€ä¸ªè¯„è®º
                if "reviewId" in item or "text" in item:
                    all_reviews.append(item)
            
            # ç¡®å®šæ˜¯å¦å·²è·å–æ‰€æœ‰å¯ç”¨é¡µé¢
            if has_current_page_info:
                fetched_all_available_reviews = (max_current_page >= MAX_PAGES_PER_ASIN or 
                                               len(all_reviews) < MAX_PAGES_PER_ASIN * DEFAULT_REVIEWS_PER_PAGE)
            else:
                expected_reviews_for_max_pages = MAX_PAGES_PER_ASIN * DEFAULT_REVIEWS_PER_PAGE
                fetched_all_available_reviews = len(all_reviews) < expected_reviews_for_max_pages
                max_current_page = min(MAX_PAGES_PER_ASIN, max(1, len(all_reviews) // DEFAULT_REVIEWS_PER_PAGE))
            
            return {
                "reviews": all_reviews,
                "total_reviews": len(all_reviews),
                "max_pages_requested": MAX_PAGES_PER_ASIN,
                "max_pages_reached": max_current_page,
                "fetched_all_available_reviews": fetched_all_available_reviews,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"   âŒ Apify API error: {str(e)}")
            return {
                "reviews": [],
                "total_reviews": 0,
                "max_pages_requested": MAX_PAGES_PER_ASIN,
                "max_pages_reached": 0,
                "fetched_all_available_reviews": False,
                "success": False,
                "error": str(e)
            }
    
    def _parse_review_date(self, date_str: str) -> Optional[datetime]:
        """è§£æè¯„è®ºæ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        # ä»Axessoæ ¼å¼ä¸­æå–æ—¥æœŸéƒ¨åˆ†: "Reviewed in [Country] on [Date]"
        if " on " in date_str:
            date_part = date_str.split(" on ")[-1].strip()
        else:
            date_part = date_str.strip()
        
        # å°è¯•å„ç§æ—¥æœŸæ ¼å¼
        date_formats = [
            "%B %d, %Y",      # "May 23, 2025"
            "%b %d, %Y",      # "May 23, 2025" (abbreviated month)
            "%m/%d/%Y",       # "05/23/2025"
            "%Y-%m-%d",       # "2025-05-23"
            "%d %B %Y",       # "23 May 2025"
            "%d %b %Y"        # "23 May 2025" (abbreviated month)
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(date_part, fmt)
            except ValueError:
                continue
        
        logger.warning(f"æ— æ³•è§£ææ—¥æœŸå­—ç¬¦ä¸²: {date_str}")
        return None
    
    def _is_valid_review_date(self, review_date_str: str) -> bool:
        """éªŒè¯è¯„è®ºæ—¥æœŸæ˜¯å¦æœ‰æ•ˆ"""
        parsed_date = self._parse_review_date(review_date_str)
        if not parsed_date:
            return False
        
        year = parsed_date.year
        return MIN_REVIEW_YEAR <= year <= MAX_REVIEW_YEAR
    
    async def _determine_skip_action(self, asin: str, review_coverage_months: int) -> Dict[str, Any]:
        """ç¡®å®šæ˜¯å¦åº”è¯¥è·³è¿‡ASINçš„çˆ¬å–"""
        existing_files = await self._get_existing_review_files(asin)
        
        if not existing_files:
            return {"should_skip": False, "reason": "no_existing_files"}
        
        # åˆ†æç°æœ‰è¯„è®º
        all_reviews_analysis = await self._analyze_all_existing_reviews(existing_files, review_coverage_months)
        
        if all_reviews_analysis["meets_coverage_requirement"]:
            if all_reviews_analysis["recent_scrape_exists"]:
                return {
                    "should_skip": True, 
                    "reason": f"recent_sufficient_coverage ({all_reviews_analysis['latest_reviews_months']:.1f} months, {all_reviews_analysis['total_reviews']} reviews)"
                }
            else:
                return {"should_skip": False, "reason": "coverage_sufficient_but_old_scrape"}
        else:
            return {"should_skip": False, "reason": f"insufficient_coverage ({all_reviews_analysis['latest_reviews_months']:.1f} months)"}
    
    async def _analyze_all_existing_reviews(self, filepaths: List[str], review_coverage_months: int) -> Dict[str, Any]:
        """åˆ†ææ‰€æœ‰ç°æœ‰è¯„è®ºæ–‡ä»¶"""
        all_reviews = []
        most_recent_scrape_date = None
        file_scrape_dates = []
        
        for filepath in filepaths:
            try:
                async with aiofiles.open(filepath, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                
                # è·å–æ–‡ä»¶çš„çˆ¬å–æ—¥æœŸ
                scrape_timestamp = data.get("scrape_context", {}).get("scraped_at")
                if scrape_timestamp:
                    try:
                        scrape_date = datetime.fromisoformat(scrape_timestamp.replace('Z', '+00:00'))
                        file_scrape_dates.append(scrape_date)
                        if not most_recent_scrape_date or scrape_date > most_recent_scrape_date:
                            most_recent_scrape_date = scrape_date
                    except ValueError:
                        logger.warning(f"æ— æ³•è§£æçˆ¬å–æ—¶é—´æˆ³: {scrape_timestamp}")
                
                # æ”¶é›†è¯„è®º
                reviews = data.get("reviews", [])
                if isinstance(reviews, list):
                    all_reviews.extend(reviews)
                
            except Exception as e:
                logger.warning(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™ {filepath}: {e}")
                continue
        
        if not all_reviews:
            return {
                "meets_coverage_requirement": False,
                "recent_scrape_exists": False,
                "latest_reviews_months": 0,
                "total_reviews": 0,
                "file_count": len(filepaths)
            }
        
        # è§£æå¹¶è¿‡æ»¤è¯„è®ºæ—¥æœŸ
        valid_review_dates = []
        for review in all_reviews:
            date_str = review.get("date", "")
            if self._is_valid_review_date(date_str):
                parsed_date = self._parse_review_date(date_str)
                if parsed_date:
                    valid_review_dates.append(parsed_date)
        
        if not valid_review_dates:
            return {
                "meets_coverage_requirement": False,
                "recent_scrape_exists": False,
                "latest_reviews_months": 0,
                "total_reviews": len(all_reviews),
                "file_count": len(filepaths)
            }
        
        # è®¡ç®—è¦†ç›–èŒƒå›´
        now = datetime.now()
        newest_review_date = max(valid_review_dates)
        oldest_review_date = min(valid_review_dates)
        
        # è®¡ç®—æœ€æ–°è¯„è®ºçš„æœˆæ•°å·®
        days_difference = (now - newest_review_date).days
        latest_reviews_months = days_difference / DAYS_PER_MONTH
        
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³è¦†ç›–è¦æ±‚
        meets_coverage_requirement = latest_reviews_months <= review_coverage_months
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ€è¿‘çš„çˆ¬å–
        recent_scrape_exists = False
        if most_recent_scrape_date:
            days_since_scrape = (now - most_recent_scrape_date).days
            recent_scrape_exists = days_since_scrape <= SCRAPE_RECENCY_DAYS
        
        return {
            "meets_coverage_requirement": meets_coverage_requirement,
            "recent_scrape_exists": recent_scrape_exists,
            "latest_reviews_months": latest_reviews_months,
            "total_reviews": len(all_reviews),
            "valid_reviews": len(valid_review_dates),
            "newest_review_date": newest_review_date.isoformat(),
            "oldest_review_date": oldest_review_date.isoformat(),
            "file_count": len(filepaths),
            "most_recent_scrape": most_recent_scrape_date.isoformat() if most_recent_scrape_date else None
        }
    
    async def _generate_filename(self, asin: str, batch_id: Optional[int] = None) -> str:
        """ç”Ÿæˆè¯„è®ºæ–‡ä»¶å"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if batch_id is not None:
            filename = f"batch_{batch_id}_{asin}_{timestamp}_reviews.json"
        else:
            filename = f"{asin}_{timestamp}_reviews.json"
        
        return os.path.join(AMAZON_REVIEW_DIR, filename)
    
    async def _save_reviews_with_context(self, asin: str, reviews_data: Dict[str, Any], 
                                       filepath: str, earliest_reviews_fetched: bool = False,
                                       max_pages_info: Optional[Dict[str, Any]] = None) -> None:
        """ä¿å­˜è¯„è®ºæ•°æ®"""
        # æ„å»ºå®Œæ•´çš„æ•°æ®ç»“æ„
        complete_data = {
            "asin": asin,
            "scrape_context": {
                "scraped_at": datetime.now().isoformat(),
                "scraper_version": "apify_axesso_v1",
                "total_reviews_fetched": reviews_data.get("total_reviews", 0),
                "earliest_reviews_fetched": earliest_reviews_fetched,
                "max_pages_info": max_pages_info or {},
                "configuration": {
                    "max_pages_per_asin": MAX_PAGES_PER_ASIN,
                    "reviews_per_page_default": DEFAULT_REVIEWS_PER_PAGE,
                    "sort_by": AMAZON_REVIEW_CONFIG["sortBy"],
                    "reviewer_type": AMAZON_REVIEW_CONFIG["reviewerType"],
                    "format_type": AMAZON_REVIEW_CONFIG["formatType"],
                    "media_type": AMAZON_REVIEW_CONFIG["mediaType"]
                }
            },
            "reviews": reviews_data.get("reviews", []),
            "api_response_metadata": {
                "success": reviews_data.get("success", False),
                "max_pages_requested": reviews_data.get("max_pages_requested", MAX_PAGES_PER_ASIN),
                "max_pages_reached": reviews_data.get("max_pages_reached", 0),
                "fetched_all_available_reviews": reviews_data.get("fetched_all_available_reviews", False)
            }
        }
        
        # å¦‚æœæœ‰é”™è¯¯ï¼Œä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
        if "error" in reviews_data:
            complete_data["api_response_metadata"]["error"] = reviews_data["error"]
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(complete_data, indent=2, ensure_ascii=False)) 